{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a3cd568-6f0c-4139-8b3b-4c3580d0762f",
   "metadata": {},
   "source": [
    "# **Level 1: The Origins — Intro to LLMs & Chatbots**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0c370f-e600-4f24-8485-ece320bf166f",
   "metadata": {},
   "source": [
    "## **Section 2: Introduction to Language Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a43ac1e-e1aa-41b4-90e4-c1a25cb63153",
   "metadata": {},
   "source": [
    "### **Part 3: Tokens & Tokenization**\n",
    "\n",
    "---\n",
    "\n",
    "Before we can understand how AI models like ChatGPT process language, we need to appreciate a simple but crucial fact: **computers don’t understand human language the way we do**.\n",
    "\n",
    "We see language as sentences, ideas, and meaning. Computers, on the other hand, deal with numbers and symbols. To bridge that gap, the first step in building modern AI systems that understand text is **breaking down language into smaller, manageable pieces**. These pieces are called **tokens**.\n",
    "\n",
    "---\n",
    "\n",
    "### **What are Tokens?**\n",
    "\n",
    "In simple terms, a **token** is a unit of text that the model processes. Depending on the model and its design, a token can be:\n",
    "\n",
    "* A full word (e.g., \"cat\")\n",
    "* Part of a word (e.g., \"inter\" and \"national\" from \"international\")\n",
    "* Punctuation (e.g., \".\")\n",
    "* Special symbols (e.g., `<|endoftext|>`)\n",
    "\n",
    "Tokens are the building blocks of language for AI models.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Not Just Use Whole Words?**\n",
    "\n",
    "Language is complex. Words can be long, short, combined, or made-up. If we treated only whole words as units, the model would struggle with:\n",
    "\n",
    "* Rare words\n",
    "* Misspellings\n",
    "* New words never seen before\n",
    "\n",
    "Instead, breaking text into smaller chunks (tokens) allows the model to handle language flexibly. Even if it has never seen the exact word \"antidisestablishmentarianism,\" it can process its tokens and still understand parts of it.\n",
    "\n",
    "---\n",
    "\n",
    "### **How is Text Broken into Tokens?**\n",
    "\n",
    "This process is called **tokenization**. A special algorithm breaks text into tokens according to predefined rules.\n",
    "\n",
    "Different models use different tokenization strategies:\n",
    "\n",
    "* Some use **WordPiece** (common in BERT models)\n",
    "* Others use **Byte Pair Encoding (BPE)** (common in GPT models)\n",
    "* Some use **SentencePiece** (common in multilingual models)\n",
    "\n",
    "These methods aim to balance efficiency and flexibility.\n",
    "\n",
    "---\n",
    "\n",
    "**Illustration Example:**\n",
    "\n",
    "Take the sentence:\n",
    "*\"I love international collaborations.\"*\n",
    "\n",
    "A tokenization algorithm might break it down like this:\n",
    "\n",
    "\\[`I`, `love`, `inter`, `national`, `collaborations`, `.`]\n",
    "\n",
    "Notice how:\n",
    "\n",
    "* \"international\" becomes two tokens: \"inter\" and \"national\"\n",
    "* Punctuation is kept as its own token\n",
    "\n",
    "Alternatively, depending on the tokenizer, it might also look like:\n",
    "\\[`I`, `love`, `international`, `collaborations`, `.`]\n",
    "\n",
    "The key takeaway: tokenization isn't always perfectly intuitive to humans, but it's optimized for the model to handle language efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Does Token Count Matter?**\n",
    "\n",
    "Modern AI models process tokens one at a time, internally converting them into numerical representations the model can work with. However, they have a **maximum token limit**, known as the **context window**.\n",
    "\n",
    "This limit defines how much text the model can handle at once. For example:\n",
    "\n",
    "* GPT-3.5 has a limit of around **4,000 tokens**\n",
    "* GPT-4 can handle up to **128,000 tokens** in some versions\n",
    "\n",
    "If your text exceeds this limit, the model will:\n",
    "\n",
    "* Truncate the beginning or end\n",
    "* Lose context\n",
    "* Be unable to process the full input\n",
    "\n",
    "This is why understanding tokens is important, especially when building applications or chatbots that work with long text.\n",
    "\n",
    "---\n",
    "\n",
    "**Real-World Implication:**\n",
    "\n",
    "Imagine you're building a chatbot to summarize legal contracts. If the contract is too long and exceeds the token limit, the chatbot won’t see the entire document — leading to incomplete or inaccurate responses.\n",
    "\n",
    "---\n",
    "\n",
    "### **Quick Clarifications:**\n",
    "\n",
    "* **Tokens ≠ Characters.** A single token might contain multiple characters, or a single character might be its own token.\n",
    "* **Token count ≠ Word count.** A sentence with five words may have 5, 7, or more tokens depending on the tokenizer.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary:**\n",
    "\n",
    "* Tokens are the basic units of text that AI models process.\n",
    "* Tokenization breaks text into these chunks.\n",
    "* The token limit defines how much information a model can process at once.\n",
    "* Understanding tokens helps you design better AI applications and prevents errors due to exceeding context limits.\n",
    "\n",
    "---\n",
    "\n",
    "In the next part, we'll build on this by exploring the **Transformer**, the engine that processes these tokens and enables models to understand language."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
